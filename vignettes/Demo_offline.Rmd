---
title: "Demo offline"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Demo offline}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
# Required

Packages required for analysis are **OneNet** plus other packages for manipulation and representation

```{r requirement}
library(rmarkdown)
library(knitr)
library(OneNet)
library(kableExtra)
library(dplyr)
library(xlsx)
```

# Pre-treatment

This offline version of the demo uses data stored in `liver`.

```{r}
data("liver")
abundances<-liver$abundances
meta<-liver$meta
taxo=liver$taxonomy%>% dplyr::select(msp_name, species)
```

Then, the `get_count_table()` function gives the MSP counting table of the project for a given prevalence threshold as well as the species prevalences. The role of the prevalence threshold is to avoid the presence of species that are not present enough to be correctly represented in the network. In the literature, in microbial networks, the value of 30% often comes up. The advantage of a high threshold is to shorten calculation time. In particular, this vignette requires very low calculation time, and therefore we impose a strict filter of prevalence greater than 90%.

The `mgs` parameter gives the taxonomy of the species in the counting table. The `sample_id` parameter (not used here) must give the "clean" samples, these are the ones which are kept for further analyses. For example for the PRJEB6337 project, sample_id is the "health_status" meta-data variable.


```{r preprocessing}
prev.min<-0.9
counts_and_species<-get_count_table(abund.table = abundances,
                                    prev.min=prev.min)
counts<-counts_and_species$data
species<-colnames(counts)
head(counts_and_species$prevalences)
```


# Frequency selection inference 

Two main steps:

1. Estimation of the regularization paths of all methods,
2. Adaptation of the average stability of all methods to the desired level.

## Estimation avec chaque méthode

Step 1 is carried out by the `all_inferences()` function. The parameters to adjust are modeling parameters (covar and offset), resampling (`rep.num`, `n.levels`, `cores`) and edge detection (`edge_thresh`). The `seed` parameter (not used here) is also available to vary the results of the SPRING method.

For this demo, we adjust the status covariate (sick “P” or healthy “H”), with offset estimation because we start from the non-rarefied table. By default the number of resamples is set to `rep.num=100`, but here for calculation time reasons `rep.num=30`. This parameter increases the precision of calculating selection frequencies.

Likewise, for inferences it is recommended to use parallelized calculation with the `cores` parameter (for example `cores=2`). By default the value is 1 to suit Windows users, and here `cores=2` to respect the rules for building a package.


```{r inference}
# run inference and save output in project file
rep.num=30; cores=2
inference_collection=all_inferences(data=counts,
                                    rep.num = rep.num, cores = cores,
                                    covar=meta)
```



## Average stability adjustment

The second step aims to adjust the stability of each method to maintain a comparable level of precision. Simulations not presented here show that the number of selected edges (or density) is an interesting intermediate quantity for finding similar levels of precision on empirical data. Consequently, the `adapt_mean_stability()` function finds the density corresponding to the average level of stability requested across all methods. The frequency vectors retained for each method are then those which best correspond to the density found.

The major parameter of `adapt_mean_stability()` is `mean.stability`, which effect can be seen on the number of edges found (graph output when `plot=TRUE`). Simulations not presented here show that increasing the average stability level makes it possible to increase the level of reproducibility of the edges found. We recommend an average stability level of 80%.

```{r adapt_mean_stab80}
#Adapt mean stability to get edges selection frequencies
adapted<-adapt_mean_stability(inference_collection, mean.stability=0.8)
final_frequences<-adapted$freqs
```

It is possible to observe the final stabilities and frequencies obtained for each method:

```{r, eval=FALSE}
adapted$stab_data 
```

```{r, echo=FALSE}
adapted$stab_data %>% kbl() %>%
  kable_styling()
```

```{r,eval=FALSE}
head(final_frequences) 
```


```{r,echo=FALSE}
head(final_frequences) %>% kbl() %>%
  kable_styling()
```


The frequencies are then aggregated with `compute_aggreg_measures()` which offers several aggregation scores (mean, norm 2, inverse variance weighting, and the number of high frequencies).

```{r, eval=FALSE}
aggreg_data=compute_aggreg_measures(final_frequences)
head(aggreg_data)  
```

```{r, echo=FALSE}
aggreg_data=compute_aggreg_measures(final_frequences)
head(aggreg_data) %>%  kbl() %>%
  kable_styling()
```


# Guilds

Once the network is inferred, it can be used to identify microbial guilds.
Guild identification is done with two approaches:
* by latent stochastic block model (SBM), which has the flexibility to identify groups with different connectivity patterns.
* by CORE-clustering, which has the particularity of identifying groups controlled in size (minimum size set at input) and in level of similarity (minimum coherence threshold within the clusters).


## SBM clustering

The `clustering_SBM()` function gives the optimal SBM clustering for scores above the threshold, and returns:

* `graph`: graph output (network)
* `species_groups`: the species present in each group (msp identifier, species name and phyllum). In tibble form if as.tibble=TRUE.
* `groups`: the group associated with each species (vector)

```{r, echo=FALSE}
species=liver$msp_set_50
aggreg_data=liver$aggreg_ill
```

```{r, echo=FALSE}
selected_taxo=left_join(data.frame(msp_name=species),taxo, by="msp_name")

clusters=clustering_SBM(edge_freq=aggreg_data$mean, selected_taxo=selected_taxo, freq.thresh = 0.8)

```


## Visualization

It is possible to visualize the network obtained, weighted by the frequencies (thickness of the edges proportional to the frequencies) or by thresholding the aggregated frequencies (binary network).
Different edge colors can be specified to spot signs of partial correlations. Likewise, the node colors make it easy to identify the groups identified by SBM.

In the example below, we represent the network of edges having a selection frequency greater than 90%, and the nodes according to the identified groups (for a frequency threshold of 80%). 


```{r, echo=FALSE}
clusters$graph
```

```{r, echo=FALSE}
head(clusters$species_groups[[1]])
```

```{r, echo=FALSE}
head(clusters$species_groups[[1]]) %>% kbl() %>%  kable_styling()
```


## Core clustering

The `clustering_CORE()` function only focuses on strong communities and isolates groups of species that are not closely related to each other in the network, and returns:

* `correlation`: the partial correlation matrix used to identify cores.
* `graph`: graph output (network).
* `species_groups`: the species present in each group (msp identifier, species name and phyllum). In tibble form if as.tibble=TRUE.
* `groups`: the group associated with each species (vector).
* `table_groups`: the list of msp names by group.



```{r, echo=FALSE}
data_prev50 <-liver$counts_prev_ill
species=liver$msp_set_50
aggreg_data=liver$aggreg_ill
```

```{r, echo=FALSE}
selected_taxo=left_join(data.frame(msp_name=species),taxo, by="msp_name")

clusters=clustering_CORE(data_prev50, aggreg_data$mean, selected_taxo=selected_taxo, min.thresh=12, freq.thresh=0.8)

```


## Visualization

It is possible to visualize the resulting network, weighted by the correlations (thickness of the edges proportional to the correlations). The node colors make it easy to identify the groups identified by the method.



```{r, echo=FALSE}
clusters$graph$graph_final
```

```{r, echo=FALSE}
head(clusters$species_groups[[1]])
```

```{r, echo=FALSE}
head(clusters$species_groups[[1]]) %>% kbl() %>%  kable_styling()
```



# Performance on simulated data

`OneNet` allows you to carry out experiments on simulated data. To do this, the package simulates data with the `new_synth_data()` function, then evaluates the performance of the inferences using the `evalQuali()` function.

The data simulation is done following the SPRING method protocol, detailed in Yoon et al. (2019). The data, jointly simulated, will best fit the marginal distributions observed in a real data set. Furthermore, this procedure reproduces the observed proportions of zero (by species only, not by individual).

```{r}
count_data<-counts_and_species$data
simulation<-new_synth_data(count_data, graph_type="cluster",n=100, plot=TRUE)
str(simulation, max.level=1)
summary(c(count_data))
summary(c(simulation$counts))
```
The summary of the two datasets are close.

Then come the inferences, here with all the available methods:

```{r, eval=FALSE}
some_inferences<-all_inferences(simulation$counts ,edge_thresh=0.9,n.levels=50,
                                rep.num=6, cores=2)
```

Finally, we evaluate the performance of the methods with evalQuali. The returned value is a table bringing together precision, recall, stability, the method in question and the lambda penalty level.

```{r, eval=FALSE}
Eval_inference<-evalQuali(some_inferences,simulation$G)
head(Eval_inference)
Eval_inference %>% ggplot(aes(TPR, PPV, color=method))+geom_point()+geom_line()+theme_light()
```

